{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "import datasets\n",
    "from torch import cuda\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    "  make_inputs,\n",
    "  decode_tokens,\n",
    "  find_token_range,\n",
    "  predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oURHfJrzap1H"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model_name = \"vicuna-13b-v1.1\"\n",
    "sos_tok = False\n",
    "\n",
    "if \"13b\" in model_name or \"12b\" in model_name:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "my_device = torch.device(\"cuda:2\")\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=my_device,\n",
    ")\n",
    "mt.set_hs_patch_hooks = set_hs_patch_hooks_llama_batch\n",
    "mt.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAtiySLToTY7"
   },
   "source": [
    "# MultiHop reasoning experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PfhXcB54ap1I"
   },
   "outputs": [],
   "source": [
    "def generate_baseline_multihop(\n",
    "    mt, df, batch_size=256, max_gen_len=10,\n",
    "):\n",
    "    def _generate_baseline_single_batch(batch_df):\n",
    "        batch_size = len(batch_df)\n",
    "        cases = [(\"baseline_hop2\", \"hop2\"),\n",
    "                 (\"baseline_hop3\", \"hop3\"),\n",
    "                 (\"baseline_multihop3\", \"hop3\"),\n",
    "                ]\n",
    "        results = {}\n",
    "        for target_col, object_col in cases:\n",
    "\n",
    "            target_baseline_batch = np.array(batch_df[target_col])\n",
    "            object_batch = np.array(batch_df[object_col])\n",
    "\n",
    "\n",
    "            # Step 0: run the the model on target prompt baseline (having the subject token in input rather than patched)\n",
    "            # The goal of this step is to calculate whether the model works correctly by default, and to calculate surprisal\n",
    "            inp_target_baseline = make_inputs(mt.tokenizer, target_baseline_batch, mt.device)\n",
    "            seq_len_target_baseline = len(inp_target_baseline[\"input_ids\"][0])\n",
    "            output_target_baseline_toks = mt.model.generate(\n",
    "                inp_target_baseline[\"input_ids\"],\n",
    "                max_length=seq_len_target_baseline + max_gen_len,\n",
    "                pad_token_id=mt.model.generation_config.eos_token_id,\n",
    "            )[:, seq_len_target_baseline:]\n",
    "            generations_baseline = decode_tokens(mt.tokenizer, output_target_baseline_toks)\n",
    "            generations_baseline_txt = np.array([\" \".join(sample_gen) for sample_gen in generations_baseline])\n",
    "\n",
    "\n",
    "            is_correct_baseline = np.array([\n",
    "                (object_batch[i] in generations_baseline_txt[i] or\n",
    "                 object_batch[i].replace(\" \", \"\") in generations_baseline_txt[i].replace(\" \", \"\"))\n",
    "                for i in range(batch_size)\n",
    "            ])\n",
    "            results.update(\n",
    "                {\n",
    "                f\"generations_{target_col}\": generations_baseline_txt,\n",
    "                f\"is_correct_{target_col}\": is_correct_baseline,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return results\n",
    "\n",
    "    results = {}\n",
    "    n_batches = len(df) // batch_size\n",
    "    if len(df)%batch_size !=0:\n",
    "        n_batches +=1\n",
    "    for i in tqdm.tqdm(range(n_batches)):\n",
    "        cur_df = df.iloc[batch_size * i : batch_size * (i + 1)]\n",
    "        batch_results = _generate_baseline_single_batch(cur_df)\n",
    "        for key, value in batch_results.items():\n",
    "            if key in results:\n",
    "                results[key] = np.concatenate((results[key], value))\n",
    "            else:\n",
    "                results[key] = value\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3YQSI-MoTY8"
   },
   "source": [
    "# Experiment 1: Multihop Product Company CEO tuples\n",
    "\n",
    "This is a subset made only from (product, company) and (company, CEO) tuples from the LRE dataset.\n",
    "We only picked 3 (company, CEO) tuples, and 15 (product, company) tuples for each that the model is more likely to know the answer to.\n",
    "\n",
    "This is an exploratory experiment. There is a more complete experiment later in the colab.\n",
    "Hop 1: Product\n",
    "Hop 2: company\n",
    "Hop 3: CEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fW9dio43ap1J"
   },
   "outputs": [],
   "source": [
    "multihop_samples = {\n",
    "    (\"Satya Nadella\", \"Microsoft\"): [\"WinDbg\", \".NET Framework\", \"Internet Explorer\", \"MS-DOS\", \"Office Open XML\",\n",
    "               \"TypeScript\", \"Bing Maps Platform\", \"Outlook Express\", \"PowerShell\", \"Windows 95\",\n",
    "               \"Xbox 360\", \"Zune\", \"Visual Basic Script\", \"Virtual Hard Disk\", \"Robocopy\",\n",
    "              ],\n",
    "    (\"Tim Cook\", \"Apple\"): [\"Siri\", \"App Store\", \"CarPlay\", \"MacBook Air\", \"Xcode\",\n",
    "               \"macOS\", \"iWork\", \"Safari\", \"QuickTime\", \"TextEdit\",\n",
    "               \"WebKit\", \"QuickDraw\", \"Time Machine (macOS)\", \"MessagePad\", \"Macbook Pro\",\n",
    "              ],\n",
    "    (\"Sundar Pichai\", \"Google\"): [\"Chromecast\", \"Chromebook\", \"Wear OS\", \"G Suite\", \"Picasa\",\n",
    "                \"WebP Lossless\", \"General Transit Feed Specification Lossless\", \"Cloud Spanner\", \"Android TV\", \"Android Runtime\",\n",
    "                \"Android Jelly Bean\", \"Android Auto\", \"App Inventor\", \"Chromebook Pixel\", \"Project Ara\",\n",
    "               ]\n",
    "}\n",
    "\n",
    "def generate_multihop_data_ceo(fdir_out=\"./outputs/factual\", batch_size=512, max_gen_len=20, replace=False):\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    fname_out = \"multihop_product_company_ceo\"\n",
    "    if not replace and os.path.exists(f\"{fdir_out}/{fname_out}.pkl\"):\n",
    "        print(f\"File {fdir_out}/{fname_out}.pkl exists. Skipping generation. Reading file...\")\n",
    "        df = pd.read_pickle(os.path.join(fdir_out, f\"{fname_out}.pkl\"))\n",
    "        return df\n",
    "    prompt_source_template = \"{} was created by\"\n",
    "    prompt_target_template = \"Who is the current CEO of {}\"\n",
    "    sample_id = 0\n",
    "\n",
    "    print(\"Step 1: Prepare dataset...\")\n",
    "    records = []\n",
    "\n",
    "    for key, value in multihop_samples.items():\n",
    "        hop3, hop2 = key\n",
    "        for hop1 in value:\n",
    "            # hop1: Product\n",
    "            # hop2: Company\n",
    "            # hop3: CEO\n",
    "            records.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"prompt_source\": prompt_source_template.replace(\"{}\", hop1),\n",
    "                \"position_source\": -1, # always doing next token prediction\n",
    "                \"prompt_target\": prompt_target_template,\n",
    "                \"position_target\": -1,\n",
    "\n",
    "                \"baseline_hop2\":  f\"{hop1} was created by\", #  hop2\n",
    "                \"baseline_hop3\":  f\"Who is the current CEO of {hop2}\", # hop3\n",
    "                \"baseline_multihop3\": f\"Who is the current CEO of the company that created {hop1}\", # hop3\n",
    "\n",
    "                \"hop1\": hop1,\n",
    "                \"hop2\": hop2,\n",
    "                \"hop3\": hop3,\n",
    "            })\n",
    "            sample_id +=1\n",
    "\n",
    "    # Step 2: Compute baseline generations\n",
    "    print(\"Step 2: Compute baseline generations...\")\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    eval_results = generate_baseline_multihop(mt, df, batch_size=batch_size, max_gen_len=max_gen_len)\n",
    "    for key, value in eval_results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, f\"{fname_out}.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, f\"{fname_out}.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ax2wLmxap1J"
   },
   "outputs": [],
   "source": [
    "multihop_df = generate_multihop_data_ceo(batch_size=128, max_gen_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uklW1xTap1K"
   },
   "outputs": [],
   "source": [
    "def evaluate_attriburte_exraction_batch_multihop(\n",
    "    mt, df, batch_size=256, max_gen_len=10, transform=None\n",
    "):\n",
    "    def _evaluate_attriburte_exraction_single_batch(batch_df):\n",
    "        batch_size = len(batch_df)\n",
    "        prompt_source_batch = np.array(batch_df[\"prompt_source\"])\n",
    "        prompt_target_batch = np.array(batch_df[\"prompt_target\"])\n",
    "        layer_source_batch = np.array(batch_df[\"layer_source\"])\n",
    "        layer_target_batch = np.array(batch_df[\"layer_target\"])\n",
    "        position_source_batch = np.array(batch_df[\"position_source\"])\n",
    "        position_target_batch = np.array(batch_df[\"position_target\"])\n",
    "\n",
    "        object_batch = np.array(batch_df[\"hop3\"])\n",
    "\n",
    "\n",
    "        # Adjust position_target to be absolute rather than relative\n",
    "        inp_target = make_inputs(mt.tokenizer, prompt_target_batch, mt.device)\n",
    "        for i in range(batch_size):\n",
    "            if position_target_batch[i] < 0:\n",
    "                position_target_batch[i] += len(inp_target[\"input_ids\"][i])\n",
    "\n",
    "        # Step 1: run the the model on source without patching and get the hidden representations.\n",
    "        inp_source = make_inputs(mt.tokenizer, prompt_source_batch, mt.device)\n",
    "        output_orig = mt.model(**inp_source, output_hidden_states=True)\n",
    "\n",
    "        # hidden_states size (n_layers, n_sample, seq_len, hidden_dim)\n",
    "        hidden_rep = [\n",
    "            output_orig.hidden_states[layer_source_batch[i] + 1][i][\n",
    "                position_source_batch[i]\n",
    "            ]\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        if transform is not None:\n",
    "            for i in range(batch_size):\n",
    "                hidden_rep[i] = transform(hidden_rep[i])\n",
    "\n",
    "        # Step 2: do second run on target prompt, while patching the input hidden state.\n",
    "        hs_patch_config = [\n",
    "            {\n",
    "                \"batch_idx\": i,\n",
    "                \"layer_target\": layer_target_batch[i],\n",
    "                \"position_target\": position_target_batch[i],\n",
    "                \"hidden_rep\": hidden_rep[i],\n",
    "                \"skip_final_ln\": (\n",
    "                    layer_source_batch[i]\n",
    "                    == layer_target_batch[i]\n",
    "                    == mt.num_layers - 1\n",
    "                ),\n",
    "            }\n",
    "            for i in range(batch_size)\n",
    "        ]\n",
    "        patch_hooks = mt.set_hs_patch_hooks(\n",
    "            mt.model, hs_patch_config, patch_input=False, generation_mode=True\n",
    "        )\n",
    "\n",
    "        output = mt.model(**inp_target)\n",
    "\n",
    "        # NOTE: inputs are left padded,\n",
    "        # and sequence length is the same across batch\n",
    "        seq_len = len(inp_target[\"input_ids\"][0])\n",
    "        output_toks = mt.model.generate(\n",
    "            inp_target[\"input_ids\"],\n",
    "            max_length=seq_len + max_gen_len,\n",
    "            pad_token_id=mt.model.generation_config.eos_token_id,\n",
    "        )[:, seq_len:]\n",
    "        generations_patched = decode_tokens(mt.tokenizer, output_toks)\n",
    "        generations_patched_txt = np.array([\n",
    "            \" \".join(generations_patched[i])\n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "        is_correct_patched = np.array([\n",
    "            (object_batch[i] in generations_patched_txt[i]\n",
    "             or object_batch[i].replace(\" \", \"\") in generations_patched_txt[i].replace(\" \", \"\"))\n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "\n",
    "        # remove patching hooks\n",
    "        remove_hooks(patch_hooks)\n",
    "\n",
    "        cpu_hidden_rep = np.array([hidden_rep[i].detach().cpu().numpy() for i in range(batch_size)])\n",
    "\n",
    "        results = {\n",
    "            \"generations_patched\": generations_patched,\n",
    "            \"is_correct_patched\": is_correct_patched,\n",
    "            \"hidden_rep\": cpu_hidden_rep,\n",
    "\n",
    "        }\n",
    "\n",
    "        return results\n",
    "\n",
    "    results = {}\n",
    "    n_batches = len(df) // batch_size\n",
    "    if len(df)%batch_size !=0:\n",
    "        n_batches +=1\n",
    "    for i in tqdm.tqdm(range(len(df) // batch_size)):\n",
    "        cur_df = df.iloc[batch_size * i : batch_size * (i + 1)]\n",
    "        batch_results = _evaluate_attriburte_exraction_single_batch(cur_df)\n",
    "        for key, value in batch_results.items():\n",
    "            if key in results:\n",
    "                results[key] = np.concatenate((results[key], value))\n",
    "            else:\n",
    "                results[key] = value\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWL0cllWap1K"
   },
   "outputs": [],
   "source": [
    "def run_experiment(fname_in, fdir_out, fname_out = \"multihop\", batch_size=512, n_samples=-1,\n",
    "                   save_output=True, replace=False):\n",
    "    print(f\"Running experiment on {fname_in}...\")\n",
    "    if not replace and os.path.exists(f\"{fdir_out}/{fname_out}.pkl\"):\n",
    "        print(f\"File {fdir_out}/{fname_out}.pkl exists. Skipping generation. Reading file...\")\n",
    "        results_df = pd.read_pickle(f\"{fdir_out}/{fname_out}.pkl\")\n",
    "        return results_df\n",
    "    df = pd.read_pickle(f\"{fname_in}\")\n",
    "    print(f\"\\tNumber of samples: {len(df)}\")\n",
    "\n",
    "    # BATCHED\n",
    "    batch = []\n",
    "    for layer_source in tqdm.tqdm(range(mt.num_layers)):\n",
    "        for layer_target in range(mt.num_layers):\n",
    "            for _, row in df.iterrows():\n",
    "                item = dict(row)\n",
    "                item.update({\n",
    "                    \"layer_source\": layer_source,\n",
    "                    \"layer_target\": layer_target,\n",
    "                })\n",
    "                batch.append(item)\n",
    "    experiment_df = pd.DataFrame.from_records(batch)\n",
    "\n",
    "    if n_samples > 0 and n_samples<len(experiment_df):\n",
    "        experiment_df = experiment_df.sample(n=n_samples, replace=False, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\tNumber of datapoints for patching experiment: {len(experiment_df)}\")\n",
    "\n",
    "    eval_results = evaluate_attriburte_exraction_batch_multihop(mt, experiment_df, batch_size=batch_size)\n",
    "\n",
    "    results_df = experiment_df.head(len(eval_results[\"is_correct_patched\"]))\n",
    "    for key, value in eval_results.items():\n",
    "        results_df[key] = list(value)\n",
    "\n",
    "    if save_output:\n",
    "        if not os.path.exists(fdir_out):\n",
    "            os.makedirs(fdir_out)\n",
    "        results_df.to_csv(f\"{fdir_out}/{fname_out}.tsv\", sep=\"\\t\")\n",
    "        results_df.to_pickle(f\"{fdir_out}/{fname_out}.pkl\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Wlz2wlRap1K"
   },
   "outputs": [],
   "source": [
    "run_experiment(\"./outputs/factual/multihop_product_company_ceo.pkl\",\n",
    "               \"./outputs/results/factual\",\n",
    "               fname_out = \"multihop_product_company_ceo\", batch_size=128, n_samples=-1,\n",
    "               save_output=True, replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUhtDO_Uap1K"
   },
   "source": [
    "# Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LGCuzYy7ap1K"
   },
   "outputs": [],
   "source": [
    "def probe_baseline(task_type=\"factual\", task_name=\"multihop_product_company_ceo\",\n",
    "                   fname_input=\"./outputs/preprocessed_data_ceo/factual/company_ceo.pkl\",\n",
    "                   inp_label_name=\"object\",\n",
    "                   hidden_states_dir=\"./outputs/results_ceo\",\n",
    "                   probe_res_dir=\"./outputs/probe_ceo\",\n",
    "                   label_name = \"hop3\", seed=42, n_test_samples=2, # test_ratio=0.5, n_samples=4,\n",
    "                   rewrite=False, only_correct=True):\n",
    "    fdir = f\"{probe_res_dir}/{task_type}\"\n",
    "    fname_pkl = f\"{fdir}/{task_name}_only_correct_{only_correct}.pkl\"\n",
    "    if rewrite==False and os.path.exists(fname_pkl):\n",
    "        print(f\"\\t{fname_pkl} exists. Skipping generation. Reading file...\")\n",
    "        test_df = pd.read_pickle(fname_pkl)\n",
    "        return test_df\n",
    "    print(f\"Creating {fname_pkl}...\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    fname_hidden_states = f\"{hidden_states_dir}/{task_type}/{task_name}.pkl\"\n",
    "\n",
    "    # Retrieve list of classes from inputs\n",
    "    inps_df = pd.read_pickle(fname_input)\n",
    "    classes = np.unique(inps_df[inp_label_name])\n",
    "    classes_dict = {}\n",
    "    for idx, cls in enumerate(classes):\n",
    "        classes_dict[cls] = idx\n",
    "\n",
    "    # Get saved hiddens\n",
    "    hiddens_df = pd.read_pickle(fname_hidden_states)\n",
    "    hiddens_df = hiddens_df.sample(frac=1).reset_index(drop=True)\n",
    "    if only_correct:\n",
    "        hiddens_df = hiddens_df[hiddens_df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "        hiddens_df = hiddens_df[hiddens_df[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "        if len(hiddens_df)<1:\n",
    "            print(f'\\tNo correct predictions for {fname_pkl}. Skipping...')\n",
    "            return\n",
    "    sample_ids = np.unique(hiddens_df['sample_id'])\n",
    "    if len(sample_ids)<4:\n",
    "        print(f\"\\tNot enough samples to train a probe for {fname_pkl}. Skipping...\")\n",
    "        return\n",
    "    np.random.shuffle(sample_ids)\n",
    "    test_sample_ids = sample_ids[:n_test_samples]\n",
    "    train_sample_ids = sample_ids[n_test_samples:]\n",
    "    train_df = hiddens_df[hiddens_df['sample_id'].isin(train_sample_ids)]\n",
    "    test_df = hiddens_df[hiddens_df['sample_id'].isin(test_sample_ids)]\n",
    "    xs = np.stack(hiddens_df[\"hidden_rep\"])\n",
    "    ys = np.array([classes_dict[i] for i in hiddens_df[label_name]])\n",
    "\n",
    "\n",
    "    train_xs = np.stack(train_df[\"hidden_rep\"])\n",
    "    train_ys = np.array([classes_dict[i] for i in train_df[label_name]])\n",
    "    if len(np.unique(train_ys)) < 2:\n",
    "        print(f\"\\tNot enough variety to train a probe for {fname_pkl}. Skipping...\")\n",
    "        return\n",
    "    test_xs = np.stack(test_df[\"hidden_rep\"])\n",
    "    test_ys = np.array([classes_dict[i] for i in test_df[label_name]])\n",
    "\n",
    "    clf = LogisticRegression(random_state=seed).fit(train_xs, train_ys)\n",
    "    predicted_ys = clf.predict(test_xs)\n",
    "    test_df[\"object_int\"] = test_ys\n",
    "    test_df[\"predicted_int\"] = predicted_ys\n",
    "    test_df[\"predicted\"] = classes[predicted_ys]\n",
    "    test_df[\"is_correct_probe\"] = test_ys == predicted_ys\n",
    "\n",
    "    if not os.path.exists(fdir):\n",
    "        os.makedirs(fdir)\n",
    "    test_df.to_csv(os.path.join(fdir, f\"{task_name}_only_correct_{only_correct}.tsv\"), sep=\"\\t\")\n",
    "    test_df.to_pickle(os.path.join(fdir, f\"{task_name}_only_correct_{only_correct}.pkl\"))\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4LMi-JxZap1K"
   },
   "outputs": [],
   "source": [
    "probe_baseline(task_type=\"factual\", task_name=\"multihop_product_company_ceo\",\n",
    "               fname_input=\"./preprocessed_data/factual/company_ceo.pkl\",\n",
    "               inp_label_name=\"object\",\n",
    "               hidden_states_dir=\"./outputs/results_ceo\",\n",
    "               probe_res_dir=\"./outputs/probe_ceo\",\n",
    "               label_name = \"hop3\",\n",
    "               rewrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgj0nbODap1K"
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-WLsB0Map1K"
   },
   "outputs": [],
   "source": [
    "def plot_heatmaps(task_type=\"factual\", task_name=\"multihop_product_company_ceo\", version=\"v5\", _vmin=0, _vmax=1):\n",
    "    probe_res_fname = f\"./outputs/probe_{version}/{task_type}/{task_name}_only_correct_True.pkl\"\n",
    "    probe_df = pd.read_pickle(probe_res_fname)\n",
    "    plot_ttl = f\"{task_type} : {task_name} - {model_name.strip('./')}\"\n",
    "    n_samples = len(probe_df)\n",
    "\n",
    "    heatmap_patch = probe_df.groupby(['layer_target', 'layer_source'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_patch, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl} \\npatch accuracy (# samples: {n_samples})\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    ax = sns.lineplot(data=probe_df, x=\"layer_source\", y=\"is_correct_probe\")\n",
    "    ax.set_ylim(-0.01, 1.01)\n",
    "    ax.set_title(f\"{plot_ttl} \\nprobe accuracy (# samples: {n_samples})\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2iCyXYGap1K"
   },
   "outputs": [],
   "source": [
    "def plot_patching_heatmaps(task_type=\"factual\", task_name=\"multihop_product_company_ceo\", version=\"ceo\",\n",
    "                           _vmin=0, _vmax=1):\n",
    "    patch_res_fname = f\"./outputs/results_{version}/{task_type}/{task_name}.pkl\"\n",
    "    patch_df = pd.read_pickle(patch_res_fname)\n",
    "    patch_df = patch_df[patch_df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "    patch_df = patch_df[patch_df[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "    n_samples = len(patch_df)\n",
    "    if n_samples ==0:\n",
    "        print(f\"No correct predictions for {patch_res_fname}. Skipping...\")\n",
    "        return\n",
    "    plot_ttl = f\"{task_type}: {task_name}\\n{model_name.strip('./')}\"\n",
    "    baseline_acc_multihop3 = patch_df[\"is_correct_baseline_multihop3\"].mean()*100\n",
    "    baseline_acc_hop3 = patch_df[\"is_correct_baseline_hop3\"].mean()*100\n",
    "    baseline_acc_hop2 = patch_df[\"is_correct_baseline_hop2\"].mean()*100\n",
    "\n",
    "    heatmap_patched = patch_df.groupby(['layer_target', 'layer_source'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_patched, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl}\\nPatching accuracy\\nBaseline multihop reasoning accuracy: {baseline_acc_multihop3:.2f}\\n(# samples: {n_samples})\")\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SIl8dtZap1K",
    "outputId": "5213d942-f46a-444a-9a7e-ca1004169398"
   },
   "outputs": [],
   "source": [
    "plot_patching_heatmaps(version=\"ceo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwFJIvpgap1K",
    "outputId": "cb784d31-ec17-499f-e673-33347886f9b6"
   },
   "outputs": [],
   "source": [
    "plot_heatmaps(version=\"v5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghlFBvz6ap1L"
   },
   "source": [
    "# Experiment 2 : CoT experiment subset\n",
    "\n",
    "This is a subset made only from (product, company) and (company, CEO) tuples from the LRE dataset.\n",
    "We only picked 3 (company, CEO) tuples, and 15 (product, company) tuples for each that the model is more likely to know the answer to.\n",
    "\n",
    "This is an exploratory experiment. There is a more complete experiment later in the colab.\n",
    "Hop 1: Product\n",
    "Hop 2: company\n",
    "Hop 3: CEO\n",
    "\n",
    "The difference between this and experiment 1 is in the choice of source and target prompt template. In this experiment, concatenation of source and target prompt makes a reasonable query, compared to experiment 1 where they where that wasn't the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ea11E8dnap1L"
   },
   "outputs": [],
   "source": [
    "multihop_samples = {\n",
    "    (\"Satya Nadella\", \"Microsoft\"): [\"WinDbg\", \".NET Framework\", \"Internet Explorer\", \"MS-DOS\", \"Office Open XML\",\n",
    "               \"TypeScript\", \"Bing Maps Platform\", \"Outlook Express\", \"PowerShell\", \"Windows 95\",\n",
    "               \"Xbox 360\", \"Zune\", \"Visual Basic Script\", \"Virtual Hard Disk\", \"Robocopy\",\n",
    "              ],\n",
    "    (\"Tim Cook\", \"Apple\"): [\"Siri\", \"App Store\", \"CarPlay\", \"MacBook Air\", \"Xcode\",\n",
    "               \"macOS\", \"iWork\", \"Safari\", \"QuickTime\", \"TextEdit\",\n",
    "               \"WebKit\", \"QuickDraw\", \"Time Machine (macOS)\", \"MessagePad\", \"Macbook Pro\",\n",
    "              ],\n",
    "    (\"Sundar Pichai\", \"Google\"): [\"Chromecast\", \"Chromebook\", \"Wear OS\", \"G Suite\", \"Picasa\",\n",
    "                \"WebP Lossless\", \"General Transit Feed Specification Lossless\", \"Cloud Spanner\", \"Android TV\", \"Android Runtime\",\n",
    "                \"Android Jelly Bean\", \"Android Auto\", \"App Inventor\", \"Chromebook Pixel\", \"Project Ara\",\n",
    "               ]\n",
    "}\n",
    "\n",
    "\n",
    "def generate_CoT_data_prod(fdir_out=\"./outputs/preprocessed_data_prod_CoT/factual\", batch_size=512, max_gen_len=20):\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    prompt_source_template = \"Who is the current CEO of \"\n",
    "    prompt_target_template = \"the company that created {}\"\n",
    "    sample_id = 0\n",
    "\n",
    "    print(\"Step 1: Prepare dataset...\")\n",
    "    records = []\n",
    "\n",
    "    for key, value in multihop_samples.items():\n",
    "        hop3, hop2 = key\n",
    "        for hop1 in value:\n",
    "            # hop1: Product\n",
    "            # hop2: Company\n",
    "            # hop3: CEO\n",
    "\n",
    "            records.append({\n",
    "                \"sample_id\": sample_id,\n",
    "                \"prompt_source\": prompt_source_template,\n",
    "                \"position_source\": -1, # always doing next token prediction\n",
    "                \"prompt_target\": prompt_target_template.replace(\"{}\", hop1),\n",
    "                \"position_target\": -1,\n",
    "\n",
    "                \"baseline_hop2\":  f\"the company that created {hop1}\", #  hop2\n",
    "                \"baseline_hop3\":  f\"Who is the current CEO of {hop2}\", # hop3\n",
    "                \"baseline_multihop3\": f\"Who is the current CEO of the company that created {hop1}\", # hop3\n",
    "\n",
    "                \"hop1\": hop1,\n",
    "                \"hop2\": hop2,\n",
    "                \"hop3\": hop3,\n",
    "            })\n",
    "            sample_id +=1\n",
    "\n",
    "    # Step 2: Compute baseline generations\n",
    "    print(\"Step 2: Compute baseline generations...\")\n",
    "    df = pd.DataFrame.from_records(records)\n",
    "    eval_results = generate_baseline_multihop(mt, df, batch_size=batch_size, max_gen_len=max_gen_len)\n",
    "    for key, value in eval_results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, \"multihop_product_company_ceo.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, \"multihop_product_company_ceo.pkl\"))\n",
    "\n",
    "    correct_subset = df[df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "    correct_subset = correct_subset[correct_subset[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "    correct_subset.to_csv(os.path.join(fdir_out, \"multihop_product_company_ceo_only_correct_True.tsv\"), sep=\"\\t\")\n",
    "    correct_subset.to_pickle(os.path.join(fdir_out, \"multihop_product_company_ceo_only_correct_True.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0P1kEXdFap1L",
    "outputId": "e34b34a7-d791-4994-902e-68a7b75b1e5b"
   },
   "outputs": [],
   "source": [
    "multihop2_df = generate_CoT_data_prod(batch_size=128, max_gen_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6iayMYyIap1L"
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline = run_experiment(\n",
    "    \"./outputs/preprocessed_data_prod_CoT/factual/multihop_product_company_ceo_only_correct_True.pkl\",\n",
    "    \"./outputs/results_prod_CoT/factual\",\n",
    "    fname_out = \"multihop_product_company_ceo_only_correct_True\", batch_size=128, n_samples=-1,\n",
    "    save_output=True, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jAwCFKcsap1L",
    "outputId": "3fdd5376-e0b5-4c34-db08-f1f4a50eeedd"
   },
   "outputs": [],
   "source": [
    "print(\"Base MultiHop Accuracy: \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_baseline_multihop3\"].max().reset_index()[\"is_correct_baseline_multihop3\"].mean())\n",
    "\n",
    "print(\"Patching MultiHop Accuracy: \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q40fnAT5ap1L"
   },
   "source": [
    "# Experimet 3: Main CoT experiment\n",
    "\n",
    "This is the full version, using maximal amount of data possible from LRE where a multihop question can be formed combining two single-hop questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njlCL0S_ap1L"
   },
   "outputs": [],
   "source": [
    "def generate_CoT_data_v7(fname_in=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop.pkl\",\n",
    "                         fdir_out=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop\", batch_size=512, max_gen_len=20):\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    fname_pkl = f\"{fdir_out}/combined_multihop_CoT_{model_name}_only_correct_True.pkl\"\n",
    "    if os.path.exists(fname_pkl):\n",
    "        print(f\"File {fname_pkl} exists. Skipping...\")\n",
    "        return\n",
    "\n",
    "    print(\"Step 1: Read multihop dataset created using LRE data prep...\")\n",
    "    df = pd.read_pickle(fname_in)\n",
    "\n",
    "    # Step 2: Compute baseline generations\n",
    "    print(\"Step 2: Compute baseline generations...\")\n",
    "    eval_results = generate_baseline_multihop(mt, df, batch_size=batch_size, max_gen_len=max_gen_len)\n",
    "    for key, value in eval_results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}.pkl\"))\n",
    "\n",
    "    correct_subset = df[df[\"is_correct_baseline_hop2\"]].reset_index(drop=True)\n",
    "    correct_subset = correct_subset[correct_subset[\"is_correct_baseline_hop3\"]].reset_index(drop=True)\n",
    "    correct_subset.to_csv(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}_only_correct_True.tsv\"), sep=\"\\t\")\n",
    "    correct_subset.to_pickle(os.path.join(fdir_out, f\"combined_multihop_CoT_{model_name}_only_correct_True.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fkJ0hUKlap1L"
   },
   "outputs": [],
   "source": [
    "generate_CoT_data_v7(fname_in=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop.pkl\",\n",
    "                     fdir_out=\"./outputs/preprocessed_data_LRE_CoT/factual_multihop\",\n",
    "                     batch_size=128, max_gen_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfRcN9dRap1L"
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline = run_experiment(\n",
    "    f\"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop_CoT_{model_name}_only_correct_True.pkl\",\n",
    "    \"./outputs/results_LRE_CoT/factual_multihop\",\n",
    "    fname_out = f\"combined_multihop_CoT_{model_name}_only_correct_True\", batch_size=128, n_samples=-1,\n",
    "    save_output=True, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLapdFfXap1L",
    "outputId": "c1ec5a9b-489e-440f-d325-bfa37df3d47a"
   },
   "outputs": [],
   "source": [
    "efficient_subset = cot_correct_baseline[cot_correct_baseline[\"layer_source\"]<cot_correct_baseline[\"layer_target\"]].reset_index(drop=True)\n",
    "# TODO maybe run patching for all source x target, but the killer case is when source < target\n",
    "\n",
    "print(\"Base MultiHop Accuracy: \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_baseline_multihop3\"].max().reset_index()[\"is_correct_baseline_multihop3\"].mean())\n",
    "\n",
    "print(\"General Patching MultiHop Accuracy (all source layer x target layer): \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean())\n",
    "\n",
    "\n",
    "print(\"Efficient Patching MultiHop Accuracy (source layer < target layer): \",\n",
    "      efficient_subset.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUO8JFwCoTY-",
    "outputId": "e5344cb7-20ce-4bb5-b545-d2c8bfe12f2d"
   },
   "outputs": [],
   "source": [
    "multihop_fname = \"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop_CoT_vicuna-13b-v1.1.pkl\"\n",
    "df = pd.read_pickle(multihop_fname)\n",
    "print(len(df))\n",
    "\n",
    "multihop_fname_only_correct = \"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop_CoT_vicuna-13b-v1.1_only_correct_True.pkl\"\n",
    "df_only_correct = pd.read_pickle(multihop_fname_only_correct)\n",
    "print(len(df_only_correct))\n",
    "df_only_correct.groupby(['fname_src', 'fname_target']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdcYuiPuap1L"
   },
   "outputs": [],
   "source": [
    "def plot_patching_heatmaps_from_df(patch_df, _vmin=0, _vmax=None, fname_postfix=\"\", save_output=True):\n",
    "    n_samples = len(patch_df)\n",
    "    plots_dir = \"./outputs/multihop_reasoning\"\n",
    "    if not os.path.exists(plots_dir):\n",
    "        os.makedirs(plots_dir)\n",
    "\n",
    "    baseline_acc_multihop3 = patch_df[\"is_correct_baseline_multihop3\"].mean()*100\n",
    "    patching_acc = patch_df.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean() * 100\n",
    "\n",
    "    heatmap_patched = patch_df.groupby(['layer_target', 'layer_source'])[\"is_correct_patched\"].mean().unstack()\n",
    "\n",
    "    FONT_SIZE_TITLE = 16\n",
    "    FONT_SIZE_AXIS = 15\n",
    "\n",
    "    plt.figure()\n",
    "    ax = sns.heatmap(data=heatmap_patched, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"Self-correction in Multi-hop Reasoning\\n# samples: {n_samples}\", fontsize=FONT_SIZE_TITLE)\n",
    "    plt.xlabel(\"Source Layer ($\\ell$)\", fontsize=FONT_SIZE_AXIS)\n",
    "    plt.ylabel(\"Target Layer ($\\ell^*$)\", fontsize=FONT_SIZE_AXIS)\n",
    "    plt.tight_layout()\n",
    "    if save_output:\n",
    "        fname=f\"{plots_dir}/multihop_heatmap{fname_postfix}.pdf\"\n",
    "        plt.savefig(fname, format=\"pdf\", dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(f\"{fname[:-4]}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTKqZCDQap1L",
    "outputId": "9c9f9970-463a-40b8-af8a-98540b9160c5"
   },
   "outputs": [],
   "source": [
    "plot_patching_heatmaps_from_df(efficient_subset, fname_postfix=\"_source_smaller_than_target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eG9KXpkap1L",
    "outputId": "f953a771-5302-48d1-9e3e-456c29d811c7"
   },
   "outputs": [],
   "source": [
    "plot_patching_heatmaps_from_df(cot_correct_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMKLLwoloTY_"
   },
   "source": [
    "# Experiment 4 - CoT Let's think step by step baseline. Baseline\n",
    "\n",
    "How does a \"Let's think step by step\" CoT baseline compare with the CoT Patchscope?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARIDpnXjoTY_"
   },
   "outputs": [],
   "source": [
    "def step_by_step_cot_baseline(\n",
    "    fname_in=f\"./preprocessed_data/factual_multihop/combined_multihop_CoT_{model_name}_only_correct_True.pkl\",\n",
    "    fdir_out=\"./outputs/results_CoT/factual_multihop\",\n",
    "    fname_out = f\"combined_multihop_CoT_{model_name}_only_correct_True_step_by_step\",\n",
    "    batch_size=128,\n",
    "    max_gen_len=20,\n",
    "    rewrite=False,\n",
    "    target_col = \"baseline_multihop3\",\n",
    "    object_col = \"hop3\",\n",
    "    cot_prefix = \"Let's think step by step. \"):\n",
    "\n",
    "    if not os.path.exists(fname_in):\n",
    "        print(f'File {fname_in} does not exist. Skipping...')\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(fdir_out):\n",
    "        os.makedirs(fdir_out)\n",
    "    fname_pkl = f\"{fdir_out}/{fname_out}.pkl\"\n",
    "    if rewrite==False and os.path.exists(fname_pkl):\n",
    "        print(f\"\\t{fname_pkl} exists. Skipping generation. Reading file...\")\n",
    "        df = pd.read_pickle(fname_pkl)\n",
    "        return df\n",
    "\n",
    "    print(\"Computing step-by-step baseline generations...\")\n",
    "    df = pd.read_pickle(fname_in)\n",
    "    df[\"cot_prefix\"] = cot_prefix\n",
    "\n",
    "    def _generate_baseline_single_batch(batch_df):\n",
    "        batch_size = len(batch_df)\n",
    "\n",
    "        results = {}\n",
    "        target_baseline_batch = np.array(batch_df[target_col])\n",
    "        target_baseline_batch = np.core.defchararray.add(cot_prefix, target_baseline_batch.astype(str))\n",
    "        object_batch = np.array(batch_df[object_col])\n",
    "\n",
    "        inp_target_baseline = make_inputs(mt.tokenizer, target_baseline_batch, mt.device)\n",
    "        seq_len_target_baseline = len(inp_target_baseline[\"input_ids\"][0])\n",
    "        output_target_baseline_toks = mt.model.generate(\n",
    "            inp_target_baseline[\"input_ids\"],\n",
    "            max_length=seq_len_target_baseline + max_gen_len,\n",
    "            pad_token_id=mt.model.generation_config.eos_token_id,\n",
    "        )[:, seq_len_target_baseline:]\n",
    "        generations_baseline = decode_tokens(mt.tokenizer, output_target_baseline_toks)\n",
    "        generations_baseline_txt = np.array([\" \".join(sample_gen) for sample_gen in generations_baseline])\n",
    "\n",
    "\n",
    "        is_correct_baseline = np.array([\n",
    "            (object_batch[i] in generations_baseline_txt[i] or\n",
    "             object_batch[i].replace(\" \", \"\") in generations_baseline_txt[i].replace(\" \", \"\"))\n",
    "            for i in range(batch_size)\n",
    "        ])\n",
    "        results.update(\n",
    "            {\n",
    "            f\"step_by_step_generations_{target_col}\": generations_baseline_txt,\n",
    "            f\"step_by_step_is_correct_{target_col}\": is_correct_baseline,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    results = {}\n",
    "    n_batches = len(df) // batch_size\n",
    "    if len(df)%batch_size !=0:\n",
    "        n_batches +=1\n",
    "    for i in tqdm.tqdm(range(n_batches)):\n",
    "        cur_df = df.iloc[batch_size * i : batch_size * (i + 1)]\n",
    "        batch_results = _generate_baseline_single_batch(cur_df)\n",
    "        for key, value in batch_results.items():\n",
    "            if key in results:\n",
    "                results[key] = np.concatenate((results[key], value))\n",
    "            else:\n",
    "                results[key] = value\n",
    "\n",
    "    for key, value in results.items():\n",
    "        df[key] = list(value)\n",
    "\n",
    "    df.to_csv(os.path.join(fdir_out, f\"{fname_out}.tsv\"), sep=\"\\t\")\n",
    "    df.to_pickle(os.path.join(fdir_out, f\"{fname_out}.pkl\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En7LoZD6oTY_"
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline_step_by_step_baseline = step_by_step_cot_baseline(\n",
    "    fname_in=f\"./outputs/preprocessed_data_LRE_CoT/factual_multihop/combined_multihop_CoT_{model_name}_only_correct_True.pkl\",\n",
    "    fdir_out=\"./outputs/results_LRE_CoT/factual_multihop\",\n",
    "    fname_out = f\"combined_multihop_CoT_{model_name}_only_correct_True_step_by_step\",\n",
    "    batch_size=128,\n",
    "    max_gen_len=20,\n",
    "    target_col = \"baseline_multihop3\",\n",
    "    object_col = \"hop3\",\n",
    "    cot_prefix = \"Let's think step by step. \",\n",
    "    rewrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WNq74QGeoTY_",
    "outputId": "c7d1f731-13a5-40f8-c3a4-f3cb4a69a154"
   },
   "outputs": [],
   "source": [
    "print(\"Base MultiHop Accuracy: \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_baseline_multihop3\"].max().reset_index()[\"is_correct_baseline_multihop3\"].mean())\n",
    "\n",
    "print(\"General Patching MultiHop Accuracy (all source layer x target layer): \",\n",
    "      cot_correct_baseline.groupby(['sample_id'])[\"is_correct_patched\"].max().reset_index()[\"is_correct_patched\"].mean())\n",
    "\n",
    "\n",
    "print(\"Canonical CoT ('Let's think step by step. ') MultiHop Accuracy: \",\n",
    "      cot_correct_baseline_step_by_step_baseline.groupby(['sample_id'])[\"step_by_step_is_correct_baseline_multihop3\"].max().reset_index()[\"step_by_step_is_correct_baseline_multihop3\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PeaKrDMoTZC"
   },
   "outputs": [],
   "source": [
    "cot_correct_baseline_step_by_step_baseline['step_by_step_generations_baseline_multihop3']\n",
    "cot_correct_baseline_step_by_step_baseline[['baseline_multihop3', 'hop3', 'generations_baseline_multihop3', 'step_by_step_generations_baseline_multihop3']]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
