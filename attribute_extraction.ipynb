{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from torch import cuda\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    "  make_inputs,\n",
    "  decode_tokens,\n",
    "  find_token_range,\n",
    "  predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-iVlmvjRahV6"
   },
   "outputs": [],
   "source": [
    "model_to_hook = {\n",
    "    \"EleutherAI/pythia-6.9b\": set_hs_patch_hooks_neox,\n",
    "    \"EleutherAI/pythia-12b\": set_hs_patch_hooks_neox,\n",
    "    \"meta-llama/Llama-2-13b-hf\": set_hs_patch_hooks_llama,\n",
    "    \"lmsys/vicuna-7b-v1.5\": set_hs_patch_hooks_llama,\n",
    "    \"./stable-vicuna-13b\": set_hs_patch_hooks_llama,\n",
    "    \"CarperAI/stable-vicuna-13b-delta\": set_hs_patch_hooks_llama,\n",
    "    \"EleutherAI/gpt-j-6b\": set_hs_patch_hooks_gptj\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJu_u30hA9dd"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "# 0-shot with GPT-J\n",
    "model_name = \"gpt-j-6B\"\n",
    "sos_tok = False\n",
    "\n",
    "if \"13b\" in model_name or \"12b\" in model_name:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "my_device = torch.device(\"cuda:0\")\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=my_device,\n",
    ")\n",
    "mt.set_hs_patch_hooks = model_to_hook[model_name]\n",
    "mt.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ly4N9cT7ahV7"
   },
   "outputs": [],
   "source": [
    "def run_experiment(task_type, task_name, data_dir, output_dir, batch_size=512, n_samples=-1,\n",
    "                   save_output=True, replace=False, only_correct=False, is_icl=True):\n",
    "    fdir_out = f\"{output_dir}/{task_type}\"\n",
    "    fname_out = f\"{fdir_out}/{task_name}_only_correct_{only_correct}.pkl\"\n",
    "    if not replace and os.path.exists(fname_out):\n",
    "        print(f\"File {fname_out} exists. Skipping...\")\n",
    "        return\n",
    "    print(f\"Running experiment on {task_type}/{task_name}...\")\n",
    "    df = pd.read_pickle(f\"{data_dir}/{task_type}/{task_name}.pkl\")\n",
    "    if only_correct:\n",
    "        df = df[df[\"is_correct_baseline\"]].reset_index(drop=True)\n",
    "    # Dropping empty prompt sources. This is an artifact of saving and reloading inputs\n",
    "    df = df[df[\"prompt_source\"]!=\"\"].reset_index(drop=True)\n",
    "    # Dropping prompt sources with \\n. pandas read_pickle is not able to handle them properly and drops the rest of the input.\n",
    "    df = df[~df[\"prompt_source\"].str.contains('\\n')].reset_index(drop=True)\n",
    "    # After manual inspection, this example seems to have tokenization issues. Dropping.\n",
    "    if task_name == \"star_constellation\":\n",
    "        df = df[~df[\"prompt_source\"].str.contains(\"service\")].reset_index(drop=True)\n",
    "    elif task_name == \"object_superclass\":\n",
    "        df = df[~df[\"prompt_source\"].str.contains(\"Swainson â€™ s hawk and the prairie\")].reset_index(drop=True)\n",
    "    print(f\"\\tNumber of samples: {len(df)}\")\n",
    "\n",
    "    # BATCHED\n",
    "    batch = []\n",
    "    for _, row in tqdm.tqdm(df.iterrows()):\n",
    "        for layer_source in range(mt.num_layers-1):\n",
    "            for layer_target in range(mt.num_layers-1):\n",
    "                item = dict(row)\n",
    "                item.update({\n",
    "                    \"layer_source\": layer_source,\n",
    "                    \"layer_target\": layer_target,\n",
    "                })\n",
    "                batch.append(item)\n",
    "    experiment_df = pd.DataFrame.from_records(batch)\n",
    "\n",
    "    if n_samples > 0 and n_samples<len(experiment_df):\n",
    "        experiment_df = experiment_df.sample(n=n_samples, replace=False, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\tNumber of datapoints for patching experiment: {len(experiment_df)}\")\n",
    "\n",
    "    eval_results = evaluate_attriburte_exraction_batch(mt, experiment_df, batch_size=batch_size, is_icl=is_icl)\n",
    "\n",
    "    results_df = experiment_df.head(len(eval_results[\"is_correct_patched\"]))\n",
    "    for key, value in eval_results.items():\n",
    "        results_df[key] = list(value)\n",
    "\n",
    "    if save_output:\n",
    "        fdir_out = f\"{output_dir}/{task_type}\"\n",
    "        if not os.path.exists(fdir_out):\n",
    "            os.makedirs(fdir_out)\n",
    "        results_df.to_csv(f\"{fdir_out}/{task_name}_only_correct_{only_correct}.tsv\", sep=\"\\t\")\n",
    "        results_df.to_pickle(f\"{fdir_out}/{task_name}_only_correct_{only_correct}.pkl\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VuZ7PRIahV8"
   },
   "outputs": [],
   "source": [
    "for task_type in [\"commonsense_updated_target_prompts\", \"factual_updated_target_prompts\"]:\n",
    "    for fname in tqdm.tqdm(os.listdir(f\"./preprocessed_data/{task_type}\")):\n",
    "        if fname.endswith('.pkl'):\n",
    "            task_name = fname[:-4]\n",
    "        else:\n",
    "            continue\n",
    "        print(f\"Processing {fname}...\")\n",
    "        try:\n",
    "            run_experiment(task_type, task_name,\n",
    "                           data_dir=\"./preprocessed_data\",\n",
    "                           output_dir=f\"./outputs/results_ae\",\n",
    "                           batch_size=512,\n",
    "                           is_icl=False,\n",
    "                           only_correct=True,\n",
    "                           replace=False,\n",
    "                          )\n",
    "        except:\n",
    "            pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShCgKsWwijCx"
   },
   "source": [
    "## Plot\n",
    "heatmaps conditional on correct base model prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T3Fk_PRrijCx"
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(fname, _vmin=0, _vmax=1):\n",
    "    df = pd.read_pickle(fname)\n",
    "    plot_ttl = f\"{fname}\\n{model_name.strip('./')}\"\n",
    "\n",
    "    sub_df = df[df[\"is_correct_baseline\"]].reset_index(drop=True)\n",
    "    heatmap_data_patched_given_correct_original = sub_df.groupby(['layer_source', 'layer_target'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_data_patched_given_correct_original, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl} - accuracy\\n(successful patch conditional on SUCESSFUL original)\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    sub_df_2 = df[df[\"is_correct_baseline\"]==False].reset_index(drop=True)\n",
    "    heatmap_data_patched_given_incorrect_original = sub_df_2.groupby(['layer_source', 'layer_target'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_data_patched_given_incorrect_original, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl} - accuracy\\n(successful patch conditional on UNSUCCESSFUL original)\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    heatmap_data_original = df.groupby(['layer_source', 'layer_target'])[\"is_correct_baseline\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_data_original, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl} - successful original\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "    heatmap_data_patched = df.groupby(['layer_source', 'layer_target'])[\"is_correct_patched\"].mean().unstack()\n",
    "    ax = sns.heatmap(data=heatmap_data_patched, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(f\"{plot_ttl} - successful patched\")\n",
    "    plt.show()\n",
    "\n",
    "    if \"is_correct_probe\" in df.columns:\n",
    "        heatmap_probe_correct_original = sub_df.groupby(['layer_source', 'layer_target'])[\"is_correct_probe\"].mean().unstack()\n",
    "        ax = sns.heatmap(data=heatmap_data_patched_given_correct_original, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"{plot_ttl} - accuracy\\n(probe success conditional on SUCESSFUL original)\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        heatmap_probe_given_incorrect_original = sub_df_2.groupby(['layer_source', 'layer_target'])[\"is_correct_probe\"].mean().unstack()\n",
    "        ax = sns.heatmap(data=heatmap_probe_given_incorrect_original, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"{plot_ttl} - accuracy\\n(probe success conditional on UNSUCCESSFUL original)\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        heatmap_probe = df.groupby(['layer_source', 'layer_target'])[\"is_correct_probe\"].mean().unstack()\n",
    "        ax = sns.heatmap(data=heatmap_probe, cmap=\"crest_r\", vmin=_vmin, vmax=_vmax)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"{plot_ttl} - successful probe\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I91cOksqijCx"
   },
   "outputs": [],
   "source": [
    "plot_heatmap(\"./outputs/results_ae/commonsense/fruit_inside_color.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXB5dO8oxInS"
   },
   "source": [
    "**Exp 0: Linguistic.**\n",
    "Using the source with a prompt id as target.\n",
    "\n",
    "**Exp 1: Linguistic.**\n",
    "Using the source and target that are just parapharases of each other, but similar semantically.\n",
    "\n",
    "**Exp 2: Commonsense.**\n",
    "Sampling source prompts.\n",
    "\n",
    "**Exp 3: Factual.**\n",
    "Combining different tasks to make multihop reasoning.\n",
    "Range of source should be the same as domain of target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nFVeStGxInS"
   },
   "outputs": [],
   "source": [
    "def run_experiment(task_type, task_name, batch_size=512, n_samples=-1, save_output=True):\n",
    "    print(f\"Running experiment on {task_type}/{task_name}...\")\n",
    "    df = pd.read_pickle(f\"./outputs/preprocessed_data/{task_type}/{task_name}.pkl\")\n",
    "    filtered_df = df[df[\"target_baseline_target\"] == df[\"target_baseline_prediction_gpt-j-6B\"]]\n",
    "    print(f\"\\tNumber of filtered samples: {len(filtered_df)}\")\n",
    "\n",
    "    # BATCHED\n",
    "    batch = []\n",
    "    for layer_source in tqdm.tqdm(range(mt.num_layers)):\n",
    "        for layer_target in range(mt.num_layers):\n",
    "            for _, row in filtered_df.iterrows():\n",
    "                item = dict(row)\n",
    "                item.update({\n",
    "                    \"layer_source\": layer_source,\n",
    "                    \"layer_target\": layer_target,\n",
    "                })\n",
    "                batch.append(item)\n",
    "    experiment_df = pd.DataFrame.from_records(batch)\n",
    "\n",
    "    if n_samples > 0 and n_samples<len(experiment_df):\n",
    "        experiment_df = experiment_df.sample(n=n_samples, replace=False, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\tNumber of datapoints for patching experiment: {len(experiment_df)}\")\n",
    "\n",
    "    prec_1, surprisal, next_token = evaluate_patch_next_token_prediction_batch(mt, experiment_df, batch_size=batch_size)\n",
    "\n",
    "    results_df = experiment_df.head(len(prec_1))\n",
    "    results_df['prec_1'] = prec_1\n",
    "    results_df['surprisal'] = surprisal\n",
    "    results_df['next_token'] = next_token\n",
    "\n",
    "    if save_output:\n",
    "        results_df.to_csv(f\"./outputs/results/{task_type}/{task_name}.tsv\", sep=\"\\t\")\n",
    "        results_df.to_pickle(f\"./outputs/results/{task_type}/{task_name}.pkl\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m7foAlWpxInT"
   },
   "outputs": [],
   "source": [
    "def run_experiment_prompt_id(task_type, task_name, batch_size=512, n_samples=-1, save_output=True):\n",
    "    print(f\"Running experiment on with prompt ID on {task_type}/{task_name}...\")\n",
    "    df = pd.read_pickle(f\"./outputs/preprocessed_data/{task_type}/{task_name}.pkl\")\n",
    "    df[\"prompt_target\"] = \"cat cat dog dog 1234 1234 hello hello {}\"\n",
    "    df = df.drop(['target_baseline', 'target_baseline_target', \"target_baseline_prediction_gpt-j-6B\",\n",
    "                     \"target_template_cropped_toks\"], axis=1)\n",
    "    df[\"position_target\"] = -1\n",
    "\n",
    "    # Dropping duplicate target examples, we only care about source here and want to use prompt id for target\n",
    "    df = df.drop_duplicates(subset=[\"sample_id\"])\n",
    "\n",
    "    print(f\"\\tNumber of samples: {len(df)}\")\n",
    "\n",
    "    # BATCHED\n",
    "    batch = []\n",
    "    for layer_source in tqdm.tqdm(range(mt.num_layers)):\n",
    "        for layer_target in range(mt.num_layers):\n",
    "            for _, row in df.iterrows():\n",
    "                item = dict(row)\n",
    "                item.update({\n",
    "                    \"layer_source\": layer_source,\n",
    "                    \"layer_target\": layer_target,\n",
    "                })\n",
    "                batch.append(item)\n",
    "    experiment_df = pd.DataFrame.from_records(batch)\n",
    "    if n_samples > 0 and n_samples<len(experiment_df):\n",
    "        experiment_df = experiment_df.sample(n=n_samples, replace=False, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\tNumber of datapoints for patching experiment: {len(experiment_df)}\")\n",
    "\n",
    "    prec_1, surprisal, next_token = evaluate_patch_next_token_prediction_batch(mt, experiment_df, batch_size=batch_size)\n",
    "\n",
    "    results_df = experiment_df.head(len(prec_1))\n",
    "    results_df['prec_1'] = prec_1\n",
    "    results_df['surprisal'] = surprisal\n",
    "    results_df['next_token'] = next_token\n",
    "    if save_output:\n",
    "        results_df.to_csv(f\"./outputs/results/prompt_id/{task_type}/{task_name}.tsv\", sep=\"\\t\")\n",
    "        results_df.to_pickle(f\"./outputs/results/prompt_id/{task_type}/{task_name}.pkl\")\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EFJUtf6xInT"
   },
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wpGEnK3SxInT"
   },
   "outputs": [],
   "source": [
    "def make_plots_from_df(df, plot_ttl, metrics_to_plot):\n",
    "    same_layer_df = df[df['layer_source'] == df['layer_target']].reset_index(drop=True)\n",
    "    for metric in metrics_to_plot:\n",
    "        ax = sns.lineplot(data=same_layer_df, x='layer_target', y=metric)\n",
    "        ax.set_title(f\"{plot_ttl} - source == target layer - {metric}\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        ax = sns.lineplot(data=df, x='layer_target', y=metric, hue=\"layer_source\")\n",
    "        ax.set_title(f\"{plot_ttl} - {metric}\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "        heatmap_data = df.groupby(['layer_source', 'layer_target'])[metric].mean().unstack()\n",
    "        my_cmap = \"crest\" if metric==\"surprisal\" else \"crest_r\"\n",
    "        ax = sns.heatmap(data=heatmap_data, cmap=my_cmap)\n",
    "        ax.invert_yaxis()\n",
    "        ax.set_title(f\"{plot_ttl} - {metric}\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "\n",
    "def make_plots_from_file(fname, metrics_to_plot):\n",
    "    filtered_results_df = pd.read_pickle(fname)\n",
    "    plot_ttl = f\"{fname}\\n{model_name.strip('./')}\"\n",
    "    make_plots_from_df(filtered_results_df, plot_ttl, metrics_to_plot)\n",
    "\n",
    "    return filtered_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JRQEfxtA9dh"
   },
   "source": [
    "## Exp 0 figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkRJtj0BA9dh"
   },
   "outputs": [],
   "source": [
    "# source: the opposite of **small** is\n",
    "# target: cat cat hello hello {}\n",
    "\n",
    "make_plots_from_file(\"./outputs/results/prompt_id/linguistic/adj_antonym.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TSJ9lsSbA9dh"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/prompt_id/linguistic/verb_past_tense.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2uO3JpiqA9dh"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/prompt_id/linguistic/word_first_letter.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHVj7F3uA9dh"
   },
   "source": [
    "## Exp 1 figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfBhLIGVA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/linguistic/adj_antonym.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f1ZQlZH3A9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/linguistic/verb_past_tense.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDWWx_X-A9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/linguistic/word_first_letter.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcLdtuViA9di"
   },
   "source": [
    "## Exp 2 figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9jXi3KQA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/commonsense/task_done_by_person.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7IQq2CHvA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/commonsense/task_done_by_tool.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BCtw27nSA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/commonsense/fruit_inside_color.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1e-zCHGlA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/commonsense/work_location.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YTaqQ2pA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/commonsense/substance_phase.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64PaHS5wA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/commonsense/object_superclass.pkl\", ['prec_1', 'surprisal'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfKRCrfZA9di"
   },
   "source": [
    "## Exp 3 figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lwrKSXvgA9di"
   },
   "outputs": [],
   "source": [
    "make_plots_from_file(\"./outputs/results/factual/combined_multihop.pkl\", ['prec_1', 'surprisal'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
