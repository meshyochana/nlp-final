{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GMJYfysaREkb"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdEmY4rDQ3ik",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "# Scienfitic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Visuals\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(context=\"notebook\",\n",
    "        rc={\"font.size\":16,\n",
    "            \"axes.titlesize\":16,\n",
    "            \"axes.labelsize\":16,\n",
    "            \"xtick.labelsize\": 16.0,\n",
    "            \"ytick.labelsize\": 16.0,\n",
    "            \"legend.fontsize\": 16.0})\n",
    "palette_ = sns.color_palette(\"Set1\")\n",
    "palette = palette_[2:5] + palette_[7:]\n",
    "sns.set_theme(style='whitegrid')\n",
    "\n",
    "# Utilities\n",
    "\n",
    "from general_utils import (\n",
    "  ModelAndTokenizer,\n",
    "  make_inputs,\n",
    "  decode_tokens,\n",
    "  find_token_range,\n",
    "  predict_from_input,\n",
    ")\n",
    "\n",
    "from patchscopes_utils import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQX9Qx6GGdLZ"
   },
   "outputs": [],
   "source": [
    "model_to_hook = {\n",
    "    \"EleutherAI/pythia-12b\": set_hs_patch_hooks_neox,\n",
    "    \"meta-llama/Llama-2-13b-hf\": set_hs_patch_hooks_llama,\n",
    "    \"lmsys/vicuna-7b-v1.5\": set_hs_patch_hooks_llama,\n",
    "    \"./stable-vicuna-13b\": set_hs_patch_hooks_llama,\n",
    "    \"CarperAI/stable-vicuna-13b-delta\": set_hs_patch_hooks_llama,\n",
    "    \"EleutherAI/gpt-j-6b\": set_hs_patch_hooks_gptj\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "4479f16b9a544b79bb8790693701d8de",
      "708c70001ba64b3196bf2ced4fe01f6c"
     ]
    },
    "id": "fKGGJO3GQ3in",
    "outputId": "aed82adb-d542-4de6-ade7-c2a4f7aadcc6"
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "model_name = \"EleutherAI/pythia-12b\"\n",
    "sos_tok = False\n",
    "\n",
    "if \"13b\" in model_name or \"12b\" in model_name:\n",
    "    torch_dtype = torch.float16\n",
    "else:\n",
    "    torch_dtype = None\n",
    "\n",
    "mt = ModelAndTokenizer(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch_dtype,\n",
    ")\n",
    "mt.set_hs_patch_hooks = model_to_hook[model_name]\n",
    "mt.model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qsZLB6l_GdLa"
   },
   "source": [
    "# Next token prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      ""
     ]
    },
    "id": "YdeaeZVjGdLb",
    "outputId": "4923c9ff-f31c-410f-f58f-955285533ab9"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "pile_dataset = datasets.load_from_disk('./the_pile_deduplicated')\n",
    "print(len(pile_dataset))\n",
    "pile_dataset = pile_dataset.filter(\n",
    "    lambda x: len(x['text'].split(' ')) < 250 and len(x['text']) < 2000\n",
    ").shuffle(seed=42)\n",
    "print(len(pile_dataset))\n",
    "\n",
    "trn_n = 10000\n",
    "val_n = 2000\n",
    "pile_trn = pile_dataset['text'][:trn_n]\n",
    "pile_val = pile_dataset['text'][trn_n:trn_n+val_n]\n",
    "sentences = [(x, 'train') for x in pile_trn] + [(x, 'validation') for x in pile_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtLHs-6OGdLb"
   },
   "outputs": [],
   "source": [
    "# Across layer mappings\n",
    "\n",
    "data = {}\n",
    "for sentence, split in tqdm(sentences):\n",
    "    inp = make_inputs(mt.tokenizer, [sentence], device=mt.model.device)\n",
    "    if sos_tok:\n",
    "        start_pos = 1\n",
    "    else:\n",
    "        start_pos = 0\n",
    "    position = random.randint(start_pos, len(inp['input_ids'][0]) - 1)\n",
    "\n",
    "    if (sentence, position, split) not in data:\n",
    "        output = mt.model(**inp, output_hidden_states = True)\n",
    "\n",
    "        data[(sentence, position, split)] =  [\n",
    "            output[\"hidden_states\"][layer+1][0][position].detach().cpu().numpy()\n",
    "            for layer in range(mt.num_layers)\n",
    "        ]\n",
    "\n",
    "df = pd.Series(data).reset_index()\n",
    "df.columns = ['full_text', 'position', 'data_split', 'hidden_rep']\n",
    "\n",
    "df.to_pickle(model_name+\"_pile_trn_val.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGjOSASJGdLc",
    "outputId": "d3d5c0c6-57a1-4aac-f4c3-b98a9d26bc02"
   },
   "outputs": [],
   "source": [
    "# Prompt-id mappings\n",
    "\n",
    "prompt_target = \"cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "inp_target = make_inputs(mt.tokenizer, [prompt_target], device=mt.model.device)\n",
    "\n",
    "data = {}\n",
    "for sentence, split in tqdm(sentences):\n",
    "    inp = make_inputs(mt.tokenizer, [sentence], device=mt.model.device)\n",
    "    if sos_tok:\n",
    "        start_pos = 1\n",
    "    else:\n",
    "        start_pos = 0\n",
    "    position = random.randint(start_pos, len(inp['input_ids'][0]) - 2)\n",
    "\n",
    "    if (sentence, position, split, \"source\") not in data:\n",
    "        output = mt.model(**inp, output_hidden_states = True)\n",
    "        _, answer_t = torch.max(torch.softmax(output.logits[0, -1, :], dim=0), dim=0)\n",
    "        data[(sentence, position, split, \"source\")] =  [\n",
    "            output[\"hidden_states\"][layer+1][0][position].detach().cpu().numpy()\n",
    "            for layer in range(mt.num_layers)\n",
    "        ]\n",
    "\n",
    "        inp_target['input_ids'][0][-1] = answer_t\n",
    "        output = mt.model(**inp_target, output_hidden_states = True)\n",
    "        data[(sentence, position, split, \"target\")] =  [\n",
    "            output[\"hidden_states\"][layer+1][0][-1].detach().cpu().numpy()\n",
    "            for layer in range(mt.num_layers)\n",
    "        ]\n",
    "\n",
    "df = pd.Series(data).reset_index()\n",
    "df.columns = ['full_text', 'position', 'data_split', 'prompt', 'hidden_rep']\n",
    "\n",
    "df.to_pickle(model_name+\"_pile_trn_val.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kEemdkGOGdLd"
   },
   "outputs": [],
   "source": [
    "# Pad and unpad \n",
    "\n",
    "pad = lambda x: np.hstack([x, np.ones((x.shape[0], 1))])\n",
    "unpad = lambda x: x[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_TIyUtoGdLd",
    "outputId": "a9d67b1c-3ba9-4e8b-e3d1-e45b8fdbd6c9"
   },
   "outputs": [],
   "source": [
    "# Across layer mappings\n",
    "\n",
    "output_dir = f'{model_name}_mappings_pile'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_trn = pd.DataFrame(df[df['data_split'] == 'train']['hidden_rep'].to_list(),\n",
    "                      columns=[layer for layer in range(mt.num_layers)])\n",
    "\n",
    "target_layer = mt.num_layers - 1\n",
    "Y = np.array(\n",
    "    df_trn[target_layer].values.tolist()\n",
    ")\n",
    "\n",
    "mappings = []\n",
    "for layer in range(mt.num_layers):\n",
    "    X = np.array(\n",
    "        df_trn[layer].values.tolist()\n",
    "    )\n",
    "\n",
    "    # Solve the least squares problem X * A = Y\n",
    "    # to find our transformation matrix A\n",
    "    A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n",
    "    transform = lambda x: unpad(pad(x) @ A)\n",
    "\n",
    "    mappings.append(A)\n",
    "    with open(f'{output_dir}/mapping_{layer}-{target_layer}.npy', 'wb') as fd:\n",
    "        np.save(fd, A)\n",
    "\n",
    "    print(layer, \"max error on train:\", np.abs(Y - transform(X)).max())\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BoEZwZLEGdLd",
    "outputId": "afbbe9a9-596a-4e8d-8689-96b1a2758a7a"
   },
   "outputs": [],
   "source": [
    "# Prompt-id mappings\n",
    "\n",
    "output_dir = f'{model_name}_mappings_pile_prompt-id'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "df_trn_src = pd.DataFrame(df[(df['data_split'] == 'train') & (df['prompt'] == 'source')]['hidden_rep'].to_list(),\n",
    "                          columns=[layer for layer in range(mt.num_layers)])\n",
    "df_trn_tgt = pd.DataFrame(df[(df['data_split'] == 'train') & (df['prompt'] == 'target')]['hidden_rep'].to_list(),\n",
    "                          columns=[layer for layer in range(mt.num_layers)])\n",
    "\n",
    "mappings = []\n",
    "for layer in range(mt.num_layers):\n",
    "    X = np.array(\n",
    "        df_trn_src[layer].values.tolist()\n",
    "    )\n",
    "    Y = np.array(\n",
    "        df_trn_tgt[layer].values.tolist()\n",
    "    )\n",
    "\n",
    "    # Solve the least squares problem X * A = Y\n",
    "    # to find our transformation matrix A\n",
    "    A, res, rank, s = np.linalg.lstsq(pad(X), pad(Y))\n",
    "    transform = lambda x: unpad(pad(x) @ A)\n",
    "\n",
    "    mappings.append(A)\n",
    "    with open(f'{output_dir}/mapping_{layer}.npy', 'wb') as fd:\n",
    "        np.save(fd, A)\n",
    "\n",
    "    print(layer, \"max error on train:\", np.abs(Y - transform(X)).max())\n",
    "\n",
    "shutil.make_archive(output_dir, 'zip', output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UhNsl2HGdLd"
   },
   "outputs": [],
   "source": [
    "mappings = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    with open(f'{model_name}_mappings_pile/mapping_{layer}-{mt.num_layers-1}.npy', 'rb') as fd:\n",
    "        A = np.load(fd)\n",
    "    mappings.append(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DH8FA6WsGdLe"
   },
   "outputs": [],
   "source": [
    "# Evaluate linear mappings on the validation set of WikiText\n",
    "device = mt.model.device\n",
    "target_layer = mt.num_layers - 1\n",
    "\n",
    "records = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    A = mappings[layer]\n",
    "    transform = lambda x: torch.tensor(\n",
    "        np.squeeze(\n",
    "            unpad(np.dot(\n",
    "                pad(np.expand_dims(x.detach().cpu().numpy(), 0)),\n",
    "                A\n",
    "            ))\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "    for idx, row in df[df['data_split'] == 'validation'].iterrows():\n",
    "        prompt = row['full_text']\n",
    "        position = row['position']\n",
    "        prec_1, surprisal = evaluate_patch_next_token_prediction(\n",
    "            mt, prompt, prompt, layer, target_layer,\n",
    "            position, position, position_prediction=position, transform=transform)\n",
    "\n",
    "        records.append({'layer': layer, 'prec_1': prec_1, 'surprisal': surprisal})\n",
    "\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name}_mappings_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNVyKXLAGdLe",
    "outputId": "9d8a9374-5bd5-4ad8-cadf-18831f3c8846"
   },
   "outputs": [],
   "source": [
    "# Evaluate identity mapping on the validation set of WikiText\n",
    "\n",
    "target_layer = mt.num_layers - 1\n",
    "\n",
    "records = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    for idx, row in df[df['data_split'] == 'validation'].iterrows():\n",
    "        prompt = row['full_text']\n",
    "        position = row['position']\n",
    "        prec_1, surprisal = evaluate_patch_next_token_prediction(\n",
    "            mt, prompt, prompt, layer, target_layer,\n",
    "            position, position, position_prediction=position)\n",
    "\n",
    "        records.append({'layer': layer, 'prec_1': prec_1, 'surprisal': surprisal})\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "results.to_csv(f'{model_name}_identity_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yuKvnceYGdLe"
   },
   "outputs": [],
   "source": [
    "# Evaluate the ID prompt on the validation set of WikiText (with/without mappings)\n",
    "\n",
    "device = mt.model.device\n",
    "\n",
    "prompt_target = \"cat -> cat\\n1135 -> 1135\\nhello -> hello\\n?\"\n",
    "position_target = -1\n",
    "apply_mappings = True\n",
    "\n",
    "records = []\n",
    "for layer in tqdm(range(mt.num_layers)):\n",
    "    if apply_mappings:\n",
    "        A = mappings[layer]\n",
    "        transform = lambda x: torch.tensor(\n",
    "            np.squeeze(\n",
    "                unpad(np.dot(\n",
    "                    pad(np.expand_dims(x.detach().cpu().numpy(), 0)),\n",
    "                    A\n",
    "                ))\n",
    "            )\n",
    "        ).to(device)\n",
    "    else:\n",
    "        transform = None\n",
    "\n",
    "    for idx, row in df[df['data_split'] == 'validation'].iterrows():\n",
    "        if 'prompt' in row and row['prompt'] == 'target':\n",
    "            continue\n",
    "        prompt_source = row['full_text']\n",
    "        position_source = row['position']\n",
    "        prec_1, surprisal = evaluate_patch_next_token_prediction(\n",
    "            mt, prompt_source, prompt_target, layer, layer,\n",
    "            position_source, position_target, position_prediction=position_target, transform=transform)\n",
    "\n",
    "        records.append({'layer': layer, 'prec_1': prec_1, 'surprisal': surprisal})\n",
    "\n",
    "results = pd.DataFrame.from_records(records)\n",
    "if apply_mappings:\n",
    "    results.to_csv(f'{model_name}_prompt-id-mapping_pile_eval.csv')\n",
    "else:\n",
    "    results.to_csv(f'{model_name}_prompt-id_pile_eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u4v3LlWQGdLe",
    "outputId": "e4bb6dee-10ba-4792-bf80-54299bcf66a5"
   },
   "outputs": [],
   "source": [
    "results1 = pd.read_csv(f'{model_name}_identity_pile_eval.csv')\n",
    "results1[\"variant\"] = \"identity\"\n",
    "results2 = pd.read_csv(f'{model_name}_mappings_pile_eval.csv')\n",
    "results2[\"variant\"] = \"affine mapping\"\n",
    "results3 = pd.read_csv(f'{model_name}_prompt-id_pile_eval.csv')\n",
    "results3[\"variant\"] = \"prompt id\"\n",
    "\n",
    "results = pd.concat([results1, results2, results3], ignore_index=True)\n",
    "\n",
    "for metric in ['prec_1', 'surprisal']:\n",
    "    ax = sns.lineplot(data=results, x='layer', y=metric, hue=\"variant\")\n",
    "    ax.set_title(model_name.strip('./'))\n",
    "    ax.legend_.set_title('')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "2c3ec9f9cb0aa45979d92499665f4b05f2a3528d3b2ca0efacea2020d32b93f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
